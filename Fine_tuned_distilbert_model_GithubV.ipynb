{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvHYAksv9eYp"
      },
      "source": [
        "# Model Training and Evaluation\n",
        "\n",
        "This notebook prepares the environment by installing necessary libraries, importing modules, mounting Google Drive, and setting up directories. It then loads and cleans the dataset, applying a text cleaning function and setting up label mappings. To address class imbalance, the training data is oversampled. The datasets are then tokenized using the DistilBERT tokenizer. A function for computing evaluation metrics such as accuracy, precision, recall, and F1-score is defined. The DistilBERT model for sequence classification is loaded, and training parameters are defined. The model is then initialized and trained using the Hugging Face Trainer on the prepared datasets. After training, the fine-tuned model and tokenizer are saved to Google Drive. Finally, the trained model is evaluated on the test set, a confusion matrix is generated, and training and validation metrics are visualized."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1 ‚Äì Install necessary packages (run once)\n",
        "# =========================\n",
        "!pip install transformers datasets scikit-learn wandb imblearn\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 2 ‚Äì Imports\n",
        "# =========================\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import wandb\n",
        "import os\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 3 ‚Äì Log in to Weights & Biases (run once)\n",
        "# =========================\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 4 ‚Äì Mount Google Drive (if using Colab)\n",
        "# =========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = '/content/drive/MyDrive/datasets/'\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 5 ‚Äì Load and clean dataset\n",
        "# =========================\n",
        "dataset = load_dataset(\"cnamuangtoun/resume-job-description-fit\")\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'\\b\\d{10,}\\b', '', text)\n",
        "    text = re.sub(r'\\d+\\s+\\w+\\s+(Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr)\\b', '', text, flags=re.IGNORECASE)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "label_to_id = {'No Fit': 0, 'Potential Fit': 1, 'Good Fit': 2}\n",
        "id2label = {v: k for k, v in label_to_id.items()}\n",
        "\n",
        "def preprocess_batch(batch):\n",
        "    batch['resume_text'] = [clean_text(t) for t in batch['resume_text']]\n",
        "    batch['job_description_text'] = [clean_text(t) for t in batch['job_description_text']]\n",
        "    batch['label_id'] = [label_to_id[l] for l in batch['label']]\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(preprocess_batch, batched=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 6 ‚Äì Oversample training data before tokenization\n",
        "# =========================\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(df_train[['resume_text', 'job_description_text']], df_train['label_id'])\n",
        "df_train_resampled = pd.DataFrame(X_resampled)\n",
        "df_train_resampled['label_id'] = y_resampled\n",
        "print(\"‚úÖ Resampled class distribution:\")\n",
        "print(df_train_resampled['label_id'].value_counts().sort_index().rename(index=id2label))\n",
        "df_train_resampled.to_csv(drive_path + \"resampled_train.csv\", index=False)\n",
        "\n",
        "# Convert to Hugging Face dataset\n",
        "resampled_train_dataset = Dataset.from_pandas(df_train_resampled)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 7 ‚Äì Tokenization\n",
        "# =========================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"resume_text\"],\n",
        "        example[\"job_description_text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = resampled_train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = dataset['test'].map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove original text columns\n",
        "tokenized_train = tokenized_train.remove_columns([\"resume_text\", \"job_description_text\"])\n",
        "tokenized_test = tokenized_test.remove_columns([\"resume_text\", \"job_description_text\"])\n",
        "\n",
        "# Rename columns to 'label' ensuring no conflicts\n",
        "tokenized_train = tokenized_train.rename_column(\"label_id\", \"label\")\n",
        "\n",
        "# Fix for test set: remove string 'label' if exists before renaming\n",
        "if \"label\" in tokenized_test.column_names and tokenized_test.features[\"label\"].dtype == \"string\":\n",
        "    tokenized_test = tokenized_test.remove_columns(\"label\")\n",
        "\n",
        "tokenized_test = tokenized_test.rename_column(\"label_id\", \"label\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 8 ‚Äì Define metrics function\n",
        "# =========================\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    p_class, r_class, f_class, _ = precision_recall_fscore_support(labels, preds, average=None, labels=[0, 1, 2])\n",
        "    metrics = {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "    for i, label in id2label.items():\n",
        "        key = label.replace(\" \", \"_\").lower()\n",
        "        metrics[f'precision_{key}'] = p_class[i]\n",
        "        metrics[f'recall_{key}'] = r_class[i]\n",
        "        metrics[f'f1_{key}'] = f_class[i]\n",
        "\n",
        "    step = trainer.state.global_step if hasattr(trainer.state, \"global_step\") else 0\n",
        "    print(f\"\\nüîé Step {step}\")\n",
        "    for k, v in metrics.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 9 ‚Äì Load model\n",
        "# =========================\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 10 ‚Äì Training arguments\n",
        "# =========================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_distilbert\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    do_eval=True,\n",
        "    seed=42,\n",
        "    disable_tqdm=False,\n",
        "    no_cuda=not torch.cuda.is_available(),\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"resume-fit-distilbert-final\"\n",
        ")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 11 ‚Äì Initialize Trainer\n",
        "# =========================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 12 ‚Äì Train the model\n",
        "# =========================\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 13 ‚Äì Save model and tokenizer\n",
        "# =========================\n",
        "save_path = \"/content/drive/MyDrive/datasets/fine_tuned_distilbert_model\"\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(\"‚úÖ Model and tokenizer saved to Google Drive!\")\n",
        "print(f\"Saved to: {save_path}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 14 ‚Äì Evaluation predictions\n",
        "# =========================\n",
        "eval_preds = trainer.predict(tokenized_test)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 15 ‚Äì Confusion matrix\n",
        "# =========================\n",
        "cm = confusion_matrix(eval_preds.label_ids, eval_preds.predictions.argmax(-1), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Fit\", \"Potential Fit\", \"Good Fit\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 16 ‚Äì Plot training & validation metrics\n",
        "# =========================\n",
        "history = trainer.state.log_history\n",
        "train_loss = [entry['loss'] for entry in history if 'loss' in entry]\n",
        "eval_logs = [entry for entry in history if 'eval_loss' in entry]\n",
        "eval_loss = [entry['eval_loss'] for entry in eval_logs]\n",
        "eval_accuracy = [entry['eval_accuracy'] for entry in eval_logs]\n",
        "eval_f1 = [entry['eval_f1'] for entry in eval_logs]\n",
        "steps_train = [entry['step'] for entry in history if 'loss' in entry]\n",
        "steps_eval = [entry['step'] for entry in eval_logs]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(steps_train, train_loss, label='Training Loss')\n",
        "plt.plot(steps_eval, eval_loss, label='Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(steps_eval, eval_accuracy, label='Validation Accuracy')\n",
        "plt.plot(steps_eval, eval_f1, label='Validation F1')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Validation Accuracy and F1')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 17 ‚Äì Plot per-class F1\n",
        "# =========================\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, label in id2label.items():\n",
        "    key = label.replace(\" \", \"_\").lower()\n",
        "    f1_scores = [entry.get(f'eval_f1_{key}', None) for entry in eval_logs]\n",
        "    plt.plot(steps_eval, f1_scores, label=f'F1: {label}')\n",
        "\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.title('Per-Class Validation F1 Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e4qSPGFUXtV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwS1IiNGzjGe"
      },
      "source": [
        "# Model Inference (Prediction)\n",
        "This cell loads the trained model and tokenizer from Google Drive and defines a function to predict the fit score between a resume and a job description."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Step 1 ‚Äì Setup and Imports\n",
        "# =========================\n",
        "# Install if needed (skip if already installed)\n",
        "!pip install transformers\n",
        "\n",
        "# Imports\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 2 ‚Äì Mount Google Drive\n",
        "# =========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 3 ‚Äì Load model and tokenizer\n",
        "# =========================\n",
        "model_path = \"/content/drive/MyDrive/datasets/fine_tuned_distilbert_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 4 ‚Äì Label mapping\n",
        "# =========================\n",
        "id2label = {0: \"No Fit\", 1: \"Potential Fit\", 2: \"Good Fit\"}\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 5 ‚Äì Inference function\n",
        "# =========================\n",
        "def predict_fit(resume_text, job_desc_text):\n",
        "    inputs = tokenizer(\n",
        "        resume_text,\n",
        "        job_desc_text,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_label = torch.argmax(probs, dim=1).item()\n",
        "        confidence = probs[0][pred_label].item()\n",
        "        good_fit_prob = probs[0][2].item()  # Probability of 'Good Fit'\n",
        "\n",
        "    print(\"üìÑ Resume:\")\n",
        "    print(resume_text.strip()[:200] + \"...\\n\")\n",
        "    print(\"üìù Job Description:\")\n",
        "    print(job_desc_text.strip()[:200] + \"...\\n\")\n",
        "    print(f\"üîç Predicted Fit: **{id2label[pred_label]}** with {confidence:.2%} confidence.\")\n",
        "\n",
        "    print(\"\\nüìä Probability Scores:\")\n",
        "    for i, prob in enumerate(probs[0]):\n",
        "        print(f\"  - {id2label[i]}: {prob.item():.2%}\")\n",
        "\n",
        "    print(f\"\\n‚≠ê Match Score (Good Fit Probability): {good_fit_prob:.2%}\")\n",
        "\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        \"pred_label\": pred_label,\n",
        "        \"confidence\": confidence,\n",
        "        \"good_fit_prob\": good_fit_prob\n",
        "    }\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Step 6 ‚Äì Test Samples\n",
        "# =========================\n",
        "resume_no_fit = \"\"\"\n",
        "Experienced barista with 4 years in food service. Skilled in preparing espresso-based drinks, operating POS systems, and delivering high customer satisfaction in fast-paced caf√©s. Seeking to advance in hospitality industry.\n",
        "\"\"\"\n",
        "\n",
        "job_no_fit = \"\"\"\n",
        "We are hiring a Senior Machine Learning Engineer to join our FinTech team. Required skills include Python, TensorFlow, cloud deployment (AWS or Azure), and at least 3 years of experience in ML model training and MLOps pipelines.\n",
        "\"\"\"\n",
        "\n",
        "resume_potential = \"\"\"\n",
        "Recent graduate with a degree in Data Science. Completed coursework in machine learning, statistics, and Python. Completed a capstone project building a recommendation system. Eager to apply skills in an internship setting.\n",
        "\"\"\"\n",
        "\n",
        "job_potential = \"\"\"\n",
        "Looking for a junior data analyst to support data processing and visualization efforts. Experience with SQL, data dashboards, and machine learning concepts preferred.\n",
        "\"\"\"\n",
        "\n",
        "resume_good = \"\"\"\n",
        "Data analyst with 3 years of experience analyzing customer behavior using Python, SQL, and Tableau. Delivered weekly insights to marketing team, improving campaign performance by 18%. Experienced with scikit-learn, pandas, and cloud tools.\n",
        "\"\"\"\n",
        "\n",
        "job_good = \"\"\"\n",
        "We are seeking an experienced data analyst to join our customer insights team. Must have strong skills in Python, SQL, and data visualization (Tableau or Power BI). Knowledge of machine learning libraries is a plus.\n",
        "\"\"\"\n",
        "\n",
        "# Put your samples in a list of tuples\n",
        "samples = [\n",
        "    (resume_no_fit, job_no_fit),\n",
        "    (resume_potential, job_potential),\n",
        "    (resume_good, job_good)\n",
        "]\n",
        "\n",
        "match_scores = []\n",
        "\n",
        "# Run predictions for each sample, print details, and store match scores\n",
        "for i, (res, job) in enumerate(samples, 1):\n",
        "    print(f\"\\n=== Sample {i} ===\")\n",
        "    result = predict_fit(res, job)\n",
        "    match_scores.append(result['good_fit_prob'])\n",
        "\n",
        "# Summary of match scores\n",
        "print(\"\\nSummary of Match Scores per sample:\")\n",
        "for i, score in enumerate(match_scores, 1):\n",
        "    print(f\"Sample {i} Good Fit Match Score: {score:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "200a052e-35e2-4171-d72e-62555ad38c48",
        "id": "46t8rxEE-OUJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Mounted at /content/drive\n",
            "\n",
            "=== Sample 1 ===\n",
            "üìÑ Resume:\n",
            "Experienced barista with 4 years in food service. Skilled in preparing espresso-based drinks, operating POS systems, and delivering high customer satisfaction in fast-paced caf√©s. Seeking to advance i...\n",
            "\n",
            "üìù Job Description:\n",
            "We are hiring a Senior Machine Learning Engineer to join our FinTech team. Required skills include Python, TensorFlow, cloud deployment (AWS or Azure), and at least 3 years of experience in ML model t...\n",
            "\n",
            "üîç Predicted Fit: **Potential Fit** with 62.98% confidence.\n",
            "\n",
            "üìä Probability Scores:\n",
            "  - No Fit: 36.39%\n",
            "  - Potential Fit: 62.98%\n",
            "  - Good Fit: 0.62%\n",
            "\n",
            "‚≠ê Match Score (Good Fit Probability): 0.62%\n",
            "================================================================================\n",
            "\n",
            "=== Sample 2 ===\n",
            "üìÑ Resume:\n",
            "Recent graduate with a degree in Data Science. Completed coursework in machine learning, statistics, and Python. Completed a capstone project building a recommendation system. Eager to apply skills in...\n",
            "\n",
            "üìù Job Description:\n",
            "Looking for a junior data analyst to support data processing and visualization efforts. Experience with SQL, data dashboards, and machine learning concepts preferred....\n",
            "\n",
            "üîç Predicted Fit: **Good Fit** with 49.79% confidence.\n",
            "\n",
            "üìä Probability Scores:\n",
            "  - No Fit: 25.58%\n",
            "  - Potential Fit: 24.63%\n",
            "  - Good Fit: 49.79%\n",
            "\n",
            "‚≠ê Match Score (Good Fit Probability): 49.79%\n",
            "================================================================================\n",
            "\n",
            "=== Sample 3 ===\n",
            "üìÑ Resume:\n",
            "Data analyst with 3 years of experience analyzing customer behavior using Python, SQL, and Tableau. Delivered weekly insights to marketing team, improving campaign performance by 18%. Experienced with...\n",
            "\n",
            "üìù Job Description:\n",
            "We are seeking an experienced data analyst to join our customer insights team. Must have strong skills in Python, SQL, and data visualization (Tableau or Power BI). Knowledge of machine learning libra...\n",
            "\n",
            "üîç Predicted Fit: **Good Fit** with 71.65% confidence.\n",
            "\n",
            "üìä Probability Scores:\n",
            "  - No Fit: 16.55%\n",
            "  - Potential Fit: 11.80%\n",
            "  - Good Fit: 71.65%\n",
            "\n",
            "‚≠ê Match Score (Good Fit Probability): 71.65%\n",
            "================================================================================\n",
            "\n",
            "Summary of Match Scores per sample:\n",
            "Sample 1 Good Fit Match Score: 0.62%\n",
            "Sample 2 Good Fit Match Score: 49.79%\n",
            "Sample 3 Good Fit Match Score: 71.65%\n"
          ]
        }
      ]
    }
  ]
}